{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from ultralytics.nn.autobackend import AutoBackend\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.quantization import QuantStub, DeQuantStub, fuse_modules, prepare, convert\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "os.environ[\"PYTORCH_SHOW_DISPATCH_TRACE\"] = \"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained best custom model weights\n",
    "model_weights = \"trained_weights/exp1_yolov8n_trained.pt\"\n",
    "# Select test image for model evaluation and latency check\n",
    "test_img = \"datasets/test/images/maksssksksss25.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Face-mask-detection\\datasets\\test\\images\\maksssksksss25.png: 640x480 1 with_mask, 155.6ms\n",
      "Speed: 6.0ms preprocess, 155.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mruns\\detect\\predict13\u001b[0m\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'with_mask', 1: 'without_mask', 2: 'mask_weared_incorrect'}\n",
      "obb: None\n",
      "orig_img: array([[[175, 185, 183],\n",
      "        [174, 184, 182],\n",
      "        [175, 185, 182],\n",
      "        ...,\n",
      "        [175, 185, 177],\n",
      "        [178, 188, 180],\n",
      "        [181, 190, 183]],\n",
      "\n",
      "       [[175, 185, 182],\n",
      "        [174, 184, 181],\n",
      "        [175, 184, 182],\n",
      "        ...,\n",
      "        [176, 186, 178],\n",
      "        [178, 188, 180],\n",
      "        [179, 188, 181]],\n",
      "\n",
      "       [[174, 184, 182],\n",
      "        [173, 183, 181],\n",
      "        [174, 183, 181],\n",
      "        ...,\n",
      "        [178, 188, 180],\n",
      "        [178, 187, 180],\n",
      "        [177, 187, 179]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[129, 133, 136],\n",
      "        [126, 130, 133],\n",
      "        [122, 126, 129],\n",
      "        ...,\n",
      "        [107, 108, 121],\n",
      "        [111, 112, 125],\n",
      "        [119, 121, 133]],\n",
      "\n",
      "       [[124, 127, 132],\n",
      "        [123, 127, 131],\n",
      "        [122, 125, 130],\n",
      "        ...,\n",
      "        [104, 109, 121],\n",
      "        [108, 112, 125],\n",
      "        [114, 118, 130]],\n",
      "\n",
      "       [[123, 126, 132],\n",
      "        [122, 125, 131],\n",
      "        [122, 125, 131],\n",
      "        ...,\n",
      "        [103, 108, 120],\n",
      "        [107, 112, 124],\n",
      "        [111, 115, 127]]], dtype=uint8)\n",
      "orig_shape: (400, 300)\n",
      "path: 'e:\\\\Face-mask-detection\\\\datasets\\\\test\\\\images\\\\maksssksksss25.png'\n",
      "probs: None\n",
      "save_dir: 'runs\\\\detect\\\\predict13'\n",
      "speed: {'preprocess': 5.983114242553711, 'inference': 155.58576583862305, 'postprocess': 1.9953250885009766}]\n"
     ]
    }
   ],
   "source": [
    "# Load the trained YOLOv8 model\n",
    "model = YOLO(model_weights)\n",
    "\n",
    "# Run inference on an image\n",
    "results = model(test_img, save=True)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Face-mask-detection\\datasets\\test\\images\\maksssksksss25.png: 640x480 1 with_mask, 120.7ms\n",
      "Speed: 3.0ms preprocess, 120.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Inference Latency of custom trained model: 153.59 ms\n"
     ]
    }
   ],
   "source": [
    "# Measure latency\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    results = model(test_img, save=False)\n",
    "end_time = time.time()\n",
    "\n",
    "latency = (end_time - start_time) * 1000  # Convert seconds to milliseconds\n",
    "print(f\"Inference Latency of custom trained model: {latency:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post Quantisation of trained model weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define path for saving quantized model weight\n",
    "quantized_model_weights = \"yolov8_mask_detection_quantized.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "Quantized model saved.\n"
     ]
    }
   ],
   "source": [
    "# Load the custom model\n",
    "model = AutoBackend(model_weights) \n",
    "model.eval()\n",
    "\n",
    "# Dynamically quantize the model \n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model.model, {torch.nn.Linear, torch.nn.Conv2d}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Saving both the quantized architecture and weights\n",
    "quantized_checkpoint = {\n",
    "    \"model\": quantized_model, \n",
    "    \"metadata\": {\n",
    "        \"classes\": model.names,  \n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(quantized_checkpoint, quantized_model_weights)\n",
    "print(\"Quantized model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\Face-mask-detection\\datasets\\test\\images\\maksssksksss25.png: 640x480 1 with_mask, 664.2ms\n",
      "Speed: 6.0ms preprocess, 664.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Quantized Inference Latency: 713.10 ms\n"
     ]
    }
   ],
   "source": [
    "qunatised_model = YOLO(quantized_model_weights)\n",
    "# Measure latency\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    results = qunatised_model(test_img, save=False)\n",
    "end_time = time.time()\n",
    "\n",
    "latency = (end_time - start_time) * 1000  # Convert seconds to milliseconds\n",
    "print(f\"Quantized Inference Latency: {latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Keys: dict_keys(['model', 'metadata'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DetectionModel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv(\n",
       "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Conv(\n",
       "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (2): C2f(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Conv(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (4): C2f(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0-1): 2 x Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Conv(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (6): C2f(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0-1): 2 x Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Conv(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (8): C2f(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): SPPF(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (10): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (11): Concat()\n",
       "    (12): C2f(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (14): Concat()\n",
       "    (15): C2f(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): Conv(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (17): Concat()\n",
       "    (18): C2f(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (19): Conv(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (act): SiLU(inplace=True)\n",
       "    )\n",
       "    (20): Concat()\n",
       "    (21): C2f(\n",
       "      (cv1): Conv(\n",
       "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (cv2): Conv(\n",
       "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (m): ModuleList(\n",
       "        (0): Bottleneck(\n",
       "          (cv1): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (cv2): Conv(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (22): Detect(\n",
       "      (cv2): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (cv3): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): Conv(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv(\n",
       "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (act): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (dfl): DFL(\n",
       "        (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load(quantized_model_weights, map_location=\"cpu\")\n",
    "\n",
    "# Check the keys to find the model\n",
    "print(\"Checkpoint Keys:\", checkpoint.keys())\n",
    "\n",
    "# If the model state_dict is in a different key, replace 'model' with the correct one\n",
    "if 'model' in checkpoint:\n",
    "    quantized_model = checkpoint['model']\n",
    "elif 'model_state_dict' in checkpoint:\n",
    "    quantized_model = checkpoint['model_state_dict']\n",
    "else:\n",
    "    raise KeyError(\"Model not found in checkpoint\")\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "quantized_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Latency and compare for both model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Latency: 192.49 ms\n"
     ]
    }
   ],
   "source": [
    "# Load test image and preprocess\n",
    "test_img_path = \"datasets/test/images/maksssksksss7.png\"\n",
    "image = cv2.imread(test_img_path)  \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "image = cv2.resize(image, (640, 640))  \n",
    "image = image / 255.0  \n",
    "image = np.transpose(image, (2, 0, 1))  \n",
    "image = np.expand_dims(image, axis=0)  \n",
    "image_tensor = torch.tensor(image, dtype=torch.float32)  \n",
    "\n",
    "# Measure latency\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    results = quantized_model(image_tensor)\n",
    "end_time = time.time()\n",
    "\n",
    "latency = (end_time - start_time) * 1000  \n",
    "print(f\"Inference Latency: {latency:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latency for FPS (Frames per Second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS (frames per second): 1.72\n"
     ]
    }
   ],
   "source": [
    "# Define the image transformation (resize and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((640, 640)),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n",
    "])\n",
    "\n",
    "# Function to load image and preprocess\n",
    "def load_image(img_path):\n",
    "    img = cv2.imread(img_path)  \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n",
    "    return img\n",
    "\n",
    "# List of test image paths \n",
    "test_img = [\n",
    "    \"datasets/test/images/maksssksksss7.png\",\n",
    "    \"datasets/test/images/maksssksksss25.png\",\n",
    "    \"datasets/test/images/maksssksksss36.png\",\n",
    "    # Add more image paths \n",
    "]\n",
    "\n",
    "# Start FPS calculation\n",
    "fps_start_time = time.time()\n",
    "num_frames = len(test_img) \n",
    "for img_path in test_img:\n",
    "    img = load_image(img_path)  \n",
    "    img = transform(img).unsqueeze(0) \n",
    "    \n",
    "    # Perform inference on each image\n",
    "    with torch.no_grad():  \n",
    "        quantized_model(img) \n",
    "\n",
    "fps_end_time = time.time()\n",
    "\n",
    "# Calculate FPS\n",
    "fps = num_frames / (fps_end_time - fps_start_time)\n",
    "print(f\"FPS (frames per second): {fps:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 5.96 MB\n",
      "Quantized model size: 11.64 MB\n",
      "Model size reduction: -95.13%\n"
     ]
    }
   ],
   "source": [
    "# Check the model size\n",
    "original_model_size = os.path.getsize(model_weights) / (1024 * 1024)\n",
    "quantized_model_size = os.path.getsize(quantized_model_weights) / (1024 * 1024)\n",
    "\n",
    "# Model size reduction ratio\n",
    "size_reduction_ratio = (original_model_size - quantized_model_size) / original_model_size * 100\n",
    "\n",
    "print(f\"Original model size: {original_model_size:.2f} MB\")\n",
    "print(f\"Quantized model size: {quantized_model_size:.2f} MB\")\n",
    "print(f\"Model size reduction: {size_reduction_ratio:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model weights file has to reduct after quantisation, but here its contradicting. So we ll try static post quantisation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quantized_model_weights = \"yolov8_mask_detection_static_quantized.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "Static quantized model saved.\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOv8 model\n",
    "model = AutoBackend(model_weights)\n",
    "model.eval()\n",
    "\n",
    "# Set quantization config\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare model for static quantization with the model for calibration\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "\n",
    "# Simulate calibration with a dummy input calibration\n",
    "dummy_input = torch.randn(1, 3, 640, 640)  \n",
    "with torch.no_grad():\n",
    "    for _ in range(100): \n",
    "        model(dummy_input)\n",
    "\n",
    "# Apply static quantization after calibration\n",
    "quantized_model = torch.quantization.convert(model, inplace=False)\n",
    "\n",
    "# Save the quantized model checkpoint\n",
    "quantized_checkpoint = {\n",
    "    \"model\": quantized_model,\n",
    "    \"metadata\": {\n",
    "        \"classes\": model.names  \n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(quantized_checkpoint, static_quantized_model_weights)\n",
    "\n",
    "print(\"Static quantized model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 5.96 MB\n",
      "Static Quantized model size: 3.17 MB\n",
      "Model size reduction: 46.80%\n"
     ]
    }
   ],
   "source": [
    "# Check the model size (in MB)\n",
    "original_model_size = os.path.getsize(model_weights) / (1024 * 1024) \n",
    "quantized_model_size = os.path.getsize(static_quantized_model_weights) / (1024 * 1024)\n",
    "\n",
    "# Model size reduction ratio\n",
    "size_reduction_ratio = (original_model_size - quantized_model_size) / original_model_size * 100\n",
    "\n",
    "print(f\"Original model size: {original_model_size:.2f} MB\")\n",
    "print(f\"Static Quantized model size: {quantized_model_size:.2f} MB\")\n",
    "print(f\"Model size reduction: {size_reduction_ratio:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the weight has reduced too. This way quantisation technique improves the model and makes it better for smaller devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
